{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Policy search\n",
    "Policy search RL algorithm for playing the game of Space Invaders."
   ],
   "id": "fcf4b5657b44c58f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T18:21:07.578973Z",
     "start_time": "2025-01-26T18:21:07.575307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the environment\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py) # needed to run atari games"
   ],
   "id": "30e26dd39c8a9006",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T18:21:07.591969Z",
     "start_time": "2025-01-26T18:21:07.587615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "a5d95091f713a05a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T18:21:07.611860Z",
     "start_time": "2025-01-26T18:21:07.608560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[34:194] \n",
    "    frame = frame[::2, ::2, 0] \n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    frame[frame != 0] = 1\n",
    "    return np.expand_dims(frame.astype(np.float32), axis=0)"
   ],
   "id": "fc4b747ab227f820",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T18:21:07.630668Z",
     "start_time": "2025-01-26T18:21:07.625210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 256),  # Updated input size after CNN\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return nn.Softmax(dim=-1)(x)\n",
    "\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    cumulative = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative = rewards[t] + gamma * cumulative\n",
    "        discounted_rewards[t] = cumulative\n",
    "    return discounted_rewards"
   ],
   "id": "248c0efb10505884",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-26T18:21:07.642187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def play_one_step(env, state, model):\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    action_probs = model(state)\n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action.item())\n",
    "    next_state = preprocess_frame(next_state)\n",
    "    return next_state, reward, done, log_prob\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model):\n",
    "    all_rewards = []\n",
    "    all_log_probs = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_log_probs = []\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "\n",
    "        for _ in range(n_max_steps):\n",
    "            state, reward, done, log_prob = play_one_step(env, state, model)\n",
    "            current_rewards.append(reward)\n",
    "            current_log_probs.append(log_prob)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_log_probs.append(current_log_probs)\n",
    "\n",
    "    return all_rewards, all_log_probs\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "    cumulative = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative = rewards[t] + discount_factor * cumulative\n",
    "        discounted[t] = cumulative\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / (reward_std + 1e-8) for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "# Hyperparameters\n",
    "env_name = 'ALE/SpaceInvaders-v5'\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Environment and model setup\n",
    "env = gym.make(env_name, render_mode=None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "policy = PolicyNetwork(env.action_space.n).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_log_probs = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, policy\n",
    "    )\n",
    "\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = []\n",
    "    policy_loss = 0\n",
    "    for log_probs, rewards in zip(all_log_probs, all_final_rewards):\n",
    "        log_probs_tensor = torch.stack(log_probs)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        policy_loss += -torch.sum(log_probs_tensor * rewards_tensor)\n",
    "\n",
    "    policy_loss /= n_episodes_per_update\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    mean_reward = np.mean([sum(rewards) for rewards in all_rewards])\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations}: Mean Reward = {mean_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "136ccc04e61b1ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Iteration 1/150: Mean Reward = 51.50\n",
      "Iteration 2/150: Mean Reward = 60.50\n",
      "Iteration 3/150: Mean Reward = 46.50\n",
      "Iteration 4/150: Mean Reward = 41.50\n",
      "Iteration 5/150: Mean Reward = 33.50\n",
      "Iteration 6/150: Mean Reward = 46.00\n",
      "Iteration 7/150: Mean Reward = 31.00\n",
      "Iteration 8/150: Mean Reward = 39.50\n",
      "Iteration 9/150: Mean Reward = 51.00\n",
      "Iteration 10/150: Mean Reward = 39.50\n",
      "Iteration 11/150: Mean Reward = 93.00\n",
      "Iteration 12/150: Mean Reward = 89.50\n",
      "Iteration 13/150: Mean Reward = 82.50\n",
      "Iteration 14/150: Mean Reward = 81.50\n",
      "Iteration 15/150: Mean Reward = 88.00\n",
      "Iteration 16/150: Mean Reward = 105.00\n",
      "Iteration 17/150: Mean Reward = 112.50\n",
      "Iteration 18/150: Mean Reward = 115.00\n",
      "Iteration 19/150: Mean Reward = 109.50\n",
      "Iteration 20/150: Mean Reward = 115.00\n",
      "Iteration 21/150: Mean Reward = 115.00\n",
      "Iteration 22/150: Mean Reward = 115.00\n",
      "Iteration 23/150: Mean Reward = 115.00\n",
      "Iteration 24/150: Mean Reward = 115.00\n",
      "Iteration 25/150: Mean Reward = 115.00\n",
      "Iteration 26/150: Mean Reward = 115.00\n",
      "Iteration 27/150: Mean Reward = 115.00\n",
      "Iteration 28/150: Mean Reward = 115.00\n",
      "Iteration 29/150: Mean Reward = 115.00\n",
      "Iteration 30/150: Mean Reward = 115.00\n",
      "Iteration 31/150: Mean Reward = 115.00\n",
      "Iteration 32/150: Mean Reward = 115.00\n",
      "Iteration 33/150: Mean Reward = 115.00\n",
      "Iteration 34/150: Mean Reward = 115.00\n",
      "Iteration 35/150: Mean Reward = 115.00\n",
      "Iteration 36/150: Mean Reward = 115.00\n",
      "Iteration 37/150: Mean Reward = 115.00\n",
      "Iteration 38/150: Mean Reward = 115.00\n",
      "Iteration 39/150: Mean Reward = 115.00\n",
      "Iteration 40/150: Mean Reward = 115.00\n",
      "Iteration 41/150: Mean Reward = 115.00\n",
      "Iteration 42/150: Mean Reward = 115.00\n",
      "Iteration 43/150: Mean Reward = 115.00\n",
      "Iteration 44/150: Mean Reward = 115.00\n",
      "Iteration 45/150: Mean Reward = 115.00\n",
      "Iteration 46/150: Mean Reward = 115.00\n",
      "Iteration 47/150: Mean Reward = 115.00\n",
      "Iteration 48/150: Mean Reward = 115.00\n",
      "Iteration 49/150: Mean Reward = 115.00\n",
      "Iteration 50/150: Mean Reward = 115.00\n",
      "Iteration 51/150: Mean Reward = 115.00\n",
      "Iteration 52/150: Mean Reward = 115.00\n",
      "Iteration 53/150: Mean Reward = 115.00\n",
      "Iteration 54/150: Mean Reward = 115.00\n",
      "Iteration 55/150: Mean Reward = 115.00\n",
      "Iteration 56/150: Mean Reward = 115.00\n",
      "Iteration 57/150: Mean Reward = 115.00\n",
      "Iteration 58/150: Mean Reward = 115.00\n",
      "Iteration 59/150: Mean Reward = 115.00\n",
      "Iteration 60/150: Mean Reward = 115.00\n",
      "Iteration 61/150: Mean Reward = 115.00\n",
      "Iteration 62/150: Mean Reward = 115.00\n",
      "Iteration 63/150: Mean Reward = 115.00\n",
      "Iteration 64/150: Mean Reward = 115.00\n",
      "Iteration 65/150: Mean Reward = 115.00\n",
      "Iteration 66/150: Mean Reward = 115.00\n",
      "Iteration 67/150: Mean Reward = 115.00\n",
      "Iteration 68/150: Mean Reward = 115.00\n",
      "Iteration 69/150: Mean Reward = 115.00\n",
      "Iteration 70/150: Mean Reward = 115.00\n",
      "Iteration 71/150: Mean Reward = 115.00\n",
      "Iteration 72/150: Mean Reward = 115.00\n",
      "Iteration 73/150: Mean Reward = 115.00\n",
      "Iteration 74/150: Mean Reward = 115.00\n",
      "Iteration 75/150: Mean Reward = 115.00\n",
      "Iteration 76/150: Mean Reward = 115.00\n",
      "Iteration 77/150: Mean Reward = 115.00\n",
      "Iteration 78/150: Mean Reward = 115.00\n",
      "Iteration 79/150: Mean Reward = 115.00\n",
      "Iteration 80/150: Mean Reward = 115.00\n",
      "Iteration 81/150: Mean Reward = 115.00\n",
      "Iteration 82/150: Mean Reward = 115.00\n",
      "Iteration 83/150: Mean Reward = 115.00\n",
      "Iteration 84/150: Mean Reward = 115.00\n",
      "Iteration 85/150: Mean Reward = 115.00\n",
      "Iteration 86/150: Mean Reward = 115.00\n",
      "Iteration 87/150: Mean Reward = 115.00\n",
      "Iteration 88/150: Mean Reward = 115.00\n",
      "Iteration 89/150: Mean Reward = 115.00\n",
      "Iteration 90/150: Mean Reward = 115.00\n",
      "Iteration 91/150: Mean Reward = 115.00\n",
      "Iteration 92/150: Mean Reward = 115.00\n",
      "Iteration 93/150: Mean Reward = 115.00\n",
      "Iteration 94/150: Mean Reward = 115.00\n",
      "Iteration 95/150: Mean Reward = 115.00\n",
      "Iteration 96/150: Mean Reward = 115.00\n",
      "Iteration 97/150: Mean Reward = 115.00\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zmum-env",
   "language": "python",
   "name": "zmum-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
