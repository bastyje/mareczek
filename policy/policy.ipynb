{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Policy search\n",
    "Policy search RL algorithm for playing the game of Space Invaders."
   ],
   "id": "fcf4b5657b44c58f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:37:46.389031Z",
     "start_time": "2025-01-26T12:37:46.386218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the environment\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py) # needed to run atari games"
   ],
   "id": "30e26dd39c8a9006",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ],
   "id": "a5d95091f713a05a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[34:194] \n",
    "    frame = frame[::2, ::2, 0] \n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    frame[frame != 0] = 1\n",
    "    return np.expand_dims(frame.astype(np.float32), axis=0)"
   ],
   "id": "fc4b747ab227f820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T12:38:14.760520Z",
     "start_time": "2025-01-26T12:37:46.403247Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Episode 1: Total Reward = 75.0\n",
      "Episode 2: Total Reward = 210.0\n",
      "Episode 3: Total Reward = 300.0\n",
      "Episode 4: Total Reward = 90.0\n",
      "Episode 5: Total Reward = 240.0\n",
      "Episode 6: Total Reward = 75.0\n",
      "Episode 7: Total Reward = 155.0\n",
      "Episode 8: Total Reward = 355.0\n",
      "Episode 9: Total Reward = 55.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 87\u001B[0m\n\u001B[1;32m     84\u001B[0m discounted_rewards \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(discounted_rewards, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Normalize rewards for stability\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m discounted_rewards \u001B[38;5;241m=\u001B[39m (discounted_rewards \u001B[38;5;241m-\u001B[39m discounted_rewards\u001B[38;5;241m.\u001B[39mmean()) \u001B[38;5;241m/\u001B[39m (\u001B[43mdiscounted_rewards\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-8\u001B[39m)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Compute policy gradient loss\u001B[39;00m\n\u001B[1;32m     90\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39msum(torch\u001B[38;5;241m.\u001B[39mstack(log_probs) \u001B[38;5;241m*\u001B[39m discounted_rewards)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 256),  # Updated input size after CNN\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return nn.Softmax(dim=-1)(x)\n",
    "\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    cumulative = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative = rewards[t] + gamma * cumulative\n",
    "        discounted_rewards[t] = cumulative\n",
    "    return discounted_rewards\n",
    "\n",
    "env = gym.make('ALE/SpaceInvaders-v5', render_mode=None)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "policy = PolicyNetwork(env.action_space.n).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "gamma = 0.99\n",
    "\n",
    "num_episodes = 500\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_probs = policy(state.unsqueeze(0))\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "        next_state = preprocess_frame(next_state)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32, device=device)\n",
    "    \n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "    loss = -torch.sum(torch.stack(log_probs) * discounted_rewards)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    episode_rewards.append(sum(rewards))\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {sum(rewards)}\")\n",
    "\n",
    "env.close()\n"
   ],
   "id": "136ccc04e61b1ffb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "611edb42c9c01e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a9b78c23e0ee8887",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zmum-env",
   "language": "python",
   "name": "zmum-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
